{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRDf3IiKlOce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a7a84e-94e4-4fd7-9df6-e2361b7af462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-6.5.0-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.4)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.27.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2022.10.31)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.9 underthesea-6.5.0 underthesea-core-1.0.4\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n",
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.10/dist-packages (6.5.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.4)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from underthesea) (0.9.9)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.27.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0)\n",
            "Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2022.10.31)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea\n",
        "!pip install transformers\n",
        "!pip install underthesea\n",
        "!pip install torch\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import csv\n",
        "import random\n",
        "import pickle\n",
        "from scipy.stats import linregress\n",
        "import underthesea\n",
        "import io\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "id": "0aoOdlQQLCLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl4W1z-dldKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a4b414-7b26-4c6b-ba86-4348f48a6897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "MAXLEN = 30\n",
        "TRUNCATING = 'post'\n",
        "PADDING = 'post'\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "MAX_EXAMPLES = 160000\n",
        "TRAINING_SPLIT = 0.9"
      ],
      "metadata": {
        "id": "awhQ9GYCXPLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(row):\n",
        "\n",
        "    row = re.sub(r\"[\\.,\\?]+$-\", \"\", row)\n",
        "    row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n",
        "        .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
        "        .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
        "        .replace('\"', \" \").replace(\"'\", \" \") \\\n",
        "        .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
        "        .replace(\"-\", \" \").replace(\"?\", \" \")\n",
        "\n",
        "    row = row.strip()\n",
        "    return row\n",
        "\n",
        "\n",
        "def remove_stopwords(sentence):\n",
        "\n",
        "    # List of stopwords\n",
        "    stopwords = list(pd.read_csv(\"/content/gdrive/MyDrive/codelab1/vietnamese-stopwords.txt\",header = None)[0])\n",
        "\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    words = sentence.split()\n",
        "    no_words = [w for w in words if w not in stopwords]\n",
        "    sentence = \" \".join(no_words)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def parse__data_from_file(filename):\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    reader = pd.read_csv(filename)\n",
        "    labels=reader[\"sentiment\"]\n",
        "    sentence = reader[\"sents\"].apply(remove_special_characters)\n",
        "    sentence = [str((sen)) for sen in sentence ]\n",
        "    labels=to_categorical(labels,3)\n",
        "\n",
        "    return sentence, labels"
      ],
      "metadata": {
        "id": "mN-6U1BvK_wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(sentences, labels, training_split):\n",
        "\n",
        "    train_size = int(len(sentences)*training_split)\n",
        "\n",
        "    # Split the sentences and labels into train/validation splits\n",
        "    train_sentences = sentences[:train_size]\n",
        "    train_labels = labels[:train_size]\n",
        "\n",
        "    validation_sentences =  sentences[train_size:]\n",
        "    validation_labels = labels[train_size:]\n",
        "\n",
        "    return train_sentences, validation_sentences, train_labels, validation_labels\n",
        "\n",
        "\n",
        "def fit_tokenizer(train_sentences, oov_token):\n",
        "\n",
        "    tokenizer = Tokenizer(oov_token=oov_token)\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "\n",
        "    return tokenizer\n"
      ],
      "metadata": {
        "id": "PFgwHBRBQGDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_pad_and_trunc(sentences, tokenizer, padding, truncating, maxlen):\n",
        "\n",
        "    # Convert sentences to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "    # Pad the sequences using the correct padding, truncating and maxlen\n",
        "    pad_trunc_sequences = pad_sequences(sequences,maxlen=maxlen,padding=padding,truncating=truncating)\n",
        "\n",
        "    return pad_trunc_sequences\n",
        "\n",
        "\n",
        "\n",
        "def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n",
        "        tf.keras.layers.Dropout(0,75),\n",
        "        tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        tf.keras.layers.GlobalMaxPooling1D(),\n",
        "        tf.keras.layers.Dropout(0,75),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\",\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_bi_lstm_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,activation='relu',dropout=0.5, recurrent_dropout=0.5)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\",\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_rnn_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "        tf.keras.layers.SimpleRNN(64,activation='relu',dropout=0.5),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\",\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def creat_svm_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "    model=svm.SVC(kernel='linear',C=1000)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "MCCcdD_1WhWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, labels = parse__data_from_file('/content/gdrive/MyDrive/codelab1/test_data.csv')\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
        "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
        "print(f\"First sentence has {len((sentences[0]).split())} words (after removing stopwords).\\n\")\n",
        "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
        "print(f\"The first 5 labels are {labels[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kVCh3z9ON1W",
        "outputId": "2b2a78cf-8d02-4d90-a6f9-e796cf198985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3166 sentences in the dataset.\n",
            "\n",
            "First sentence has 5 words (after removing stopwords).\n",
            "\n",
            "There are 3166 labels in the dataset.\n",
            "\n",
            "The first 5 labels are [[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[1:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D55T5zx6iVr",
        "outputId": "c08fbe28-f31b-41cc-f546-fe58df5dc89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['giáo viên rất vui tính',\n",
              " 'cô max có tâm',\n",
              " 'giảng bài thu hút   dí dỏm',\n",
              " 'giáo viên không giảng dạy kiến thức   hướng dẫn thực hành trong quá trình học',\n",
              " 'thầy dạy nhiệt tình và tâm huyết',\n",
              " 'tính điểm thi đua các nhóm',\n",
              " 'thầy nhiệt tình giảng lại cho học sinh',\n",
              " 'có đôi lúc nói hơi nhanh làm sinh viên không theo kịp',\n",
              " 'giảng dạy nhiệt tình   liên hệ thực tế khá nhiều   tương tác với sinh viên tương đối tốt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = fit_tokenizer(train_sentences,OOV_TOKEN)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "VOCAB_SIZE = len(word_index)\n",
        "\n",
        "print(f\"Vocabulary contains {VOCAB_SIZE} words\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNWouWtNPMiG",
        "outputId": "bdf334e8-b683-47eb-e760-4afb779a73ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary contains 1496 words\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pad_trunc_seq = seq_pad_and_trunc(train_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "val_pad_trunc_seq = seq_pad_and_trunc(val_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "\n",
        "print(f\"Padded and truncated training sequences have shape: {train_pad_trunc_seq.shape}\\n\")\n",
        "print(f\"Padded and truncated validation sequences have shape: {val_pad_trunc_seq.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqVeQ9xfQUhF",
        "outputId": "6d57e2d7-c3e5-4827-938a-bc848efd7b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded and truncated training sequences have shape: (2849, 30)\n",
            "\n",
            "Padded and truncated validation sequences have shape: (317, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)"
      ],
      "metadata": {
        "id": "rRLcXHLVwQfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/gdrive/MyDrive/AI & mcln & deep ln/NLP in tensorflow/glove.6B.100d.txt.zip\""
      ],
      "metadata": {
        "id": "Bfr0H_rV1GVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eff90fc-3128-4b58-e09c-c8d70c611319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/AI & mcln & deep ln/NLP in tensorflow/glove.6B.100d.txt.zip\n",
            "  inflating: glove.6B.100d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to file containing the embeddings\n",
        "GLOVE_FILE = '/content/glove.6B.100d.txt'\n",
        "\n",
        "# Initialize an empty embeddings index dictionary\n",
        "GLOVE_EMBEDDINGS = {}\n",
        "\n",
        "# Read file and fill GLOVE_EMBEDDINGS with its contents\n",
        "with open(GLOVE_FILE) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        GLOVE_EMBEDDINGS[word] = coefs"
      ],
      "metadata": {
        "id": "rJ_O1P0h1IH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        EMBEDDINGS_MATRIX[i] = embedding_vector"
      ],
      "metadata": {
        "id": "7kzGdp5T1KR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = create_model(VOCAB_SIZE, EMBEDDING_DIM, MAXLEN, EMBEDDINGS_MATRIX)\n",
        "\n",
        "history = model1.fit(train_pad_trunc_seq, train_labels, epochs=20, validation_data=(val_pad_trunc_seq, val_labels))"
      ],
      "metadata": {
        "id": "l9pmZDLuwAf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIMPLE RNN"
      ],
      "metadata": {
        "id": "yDz7ZWXTRhhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn =create_rnn_model(VOCAB_SIZE, EMBEDDING_DIM, MAXLEN, EMBEDDINGS_MATRIX)"
      ],
      "metadata": {
        "id": "tKUZp3DZR0mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = rnn.fit(train_pad_trunc_seq, train_labels, epochs=20, validation_data=(val_pad_trunc_seq, val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZr2130AR2Y0",
        "outputId": "925f8682-a5d9-4187-b7eb-aa67e62f2596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "90/90 [==============================] - 13s 35ms/step - loss: 0.8744 - accuracy: 0.5409 - val_loss: 0.8671 - val_accuracy: 0.5521\n",
            "Epoch 2/20\n",
            "90/90 [==============================] - 4s 47ms/step - loss: 0.8290 - accuracy: 0.5749 - val_loss: 0.8476 - val_accuracy: 0.5710\n",
            "Epoch 3/20\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.8262 - accuracy: 0.5844 - val_loss: 0.8675 - val_accuracy: 0.4606\n",
            "Epoch 4/20\n",
            "90/90 [==============================] - 3s 36ms/step - loss: 0.8292 - accuracy: 0.5760 - val_loss: 0.8498 - val_accuracy: 0.5615\n",
            "Epoch 5/20\n",
            "90/90 [==============================] - 3s 33ms/step - loss: 0.8033 - accuracy: 0.6188 - val_loss: 0.8473 - val_accuracy: 0.5868\n",
            "Epoch 6/20\n",
            "90/90 [==============================] - 3s 38ms/step - loss: 0.7677 - accuracy: 0.6546 - val_loss: 0.8158 - val_accuracy: 0.6593\n",
            "Epoch 7/20\n",
            "90/90 [==============================] - 6s 64ms/step - loss: 0.8232 - accuracy: 0.6020 - val_loss: 0.7704 - val_accuracy: 0.6845\n",
            "Epoch 8/20\n",
            "90/90 [==============================] - 3s 35ms/step - loss: 0.7380 - accuracy: 0.6859 - val_loss: 0.8016 - val_accuracy: 0.6593\n",
            "Epoch 9/20\n",
            "90/90 [==============================] - 5s 56ms/step - loss: 0.7291 - accuracy: 0.6701 - val_loss: 0.7588 - val_accuracy: 0.6972\n",
            "Epoch 10/20\n",
            "90/90 [==============================] - 5s 61ms/step - loss: 0.7324 - accuracy: 0.6637 - val_loss: 0.7811 - val_accuracy: 0.6467\n",
            "Epoch 11/20\n",
            "90/90 [==============================] - 4s 40ms/step - loss: 0.7356 - accuracy: 0.6795 - val_loss: 0.7794 - val_accuracy: 0.6530\n",
            "Epoch 12/20\n",
            "90/90 [==============================] - 3s 34ms/step - loss: 0.7258 - accuracy: 0.7009 - val_loss: 0.7636 - val_accuracy: 0.7003\n",
            "Epoch 13/20\n",
            "90/90 [==============================] - 3s 33ms/step - loss: 0.7098 - accuracy: 0.7062 - val_loss: 0.8481 - val_accuracy: 0.6151\n",
            "Epoch 14/20\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 0.7427 - accuracy: 0.6873 - val_loss: 0.7679 - val_accuracy: 0.6751\n",
            "Epoch 15/20\n",
            "90/90 [==============================] - 4s 49ms/step - loss: 0.7297 - accuracy: 0.6887 - val_loss: 0.7587 - val_accuracy: 0.6625\n",
            "Epoch 16/20\n",
            "90/90 [==============================] - 3s 33ms/step - loss: 0.7265 - accuracy: 0.6995 - val_loss: 0.7313 - val_accuracy: 0.7098\n",
            "Epoch 17/20\n",
            "90/90 [==============================] - 3s 33ms/step - loss: 0.7109 - accuracy: 0.7027 - val_loss: 0.7861 - val_accuracy: 0.6530\n",
            "Epoch 18/20\n",
            "90/90 [==============================] - 4s 46ms/step - loss: 0.7387 - accuracy: 0.6960 - val_loss: 0.7549 - val_accuracy: 0.6845\n",
            "Epoch 19/20\n",
            "90/90 [==============================] - 5s 59ms/step - loss: 0.7324 - accuracy: 0.6890 - val_loss: 0.8014 - val_accuracy: 0.6372\n",
            "Epoch 20/20\n",
            "90/90 [==============================] - 3s 33ms/step - loss: 0.7584 - accuracy: 0.6564 - val_loss: 0.7413 - val_accuracy: 0.6688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BI directional"
      ],
      "metadata": {
        "id": "gh4mTxeMOLi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bi_lstm =create_model(VOCAB_SIZE, EMBEDDING_DIM, MAXLEN, EMBEDDINGS_MATRIX)"
      ],
      "metadata": {
        "id": "dkuKU2bx_i9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = Bi_lstm.fit(train_pad_trunc_seq, train_labels, epochs=20, validation_data=(val_pad_trunc_seq, val_labels))"
      ],
      "metadata": {
        "id": "y2RqBqldNV5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc18c366-c2d3-412e-876c-ea9352d3914b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "90/90 [==============================] - 6s 17ms/step - loss: 0.7179 - accuracy: 0.6697 - val_loss: 0.6282 - val_accuracy: 0.7382\n",
            "Epoch 2/20\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 0.5938 - accuracy: 0.7427 - val_loss: 0.5766 - val_accuracy: 0.7382\n",
            "Epoch 3/20\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 0.5406 - accuracy: 0.7673 - val_loss: 0.5651 - val_accuracy: 0.7539\n",
            "Epoch 4/20\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 0.4993 - accuracy: 0.7908 - val_loss: 0.5586 - val_accuracy: 0.7476\n",
            "Epoch 5/20\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 0.4356 - accuracy: 0.8157 - val_loss: 0.6537 - val_accuracy: 0.7382\n",
            "Epoch 6/20\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 0.3873 - accuracy: 0.8459 - val_loss: 0.6292 - val_accuracy: 0.7161\n",
            "Epoch 7/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 0.3716 - accuracy: 0.8424 - val_loss: 0.6163 - val_accuracy: 0.7476\n",
            "Epoch 8/20\n",
            "90/90 [==============================] - 1s 12ms/step - loss: 0.2987 - accuracy: 0.8757 - val_loss: 0.6897 - val_accuracy: 0.7729\n",
            "Epoch 9/20\n",
            "90/90 [==============================] - 1s 12ms/step - loss: 0.2426 - accuracy: 0.8989 - val_loss: 0.7806 - val_accuracy: 0.7729\n",
            "Epoch 10/20\n",
            "90/90 [==============================] - 1s 13ms/step - loss: 0.2164 - accuracy: 0.9063 - val_loss: 0.9161 - val_accuracy: 0.7224\n",
            "Epoch 11/20\n",
            "90/90 [==============================] - 1s 13ms/step - loss: 0.1948 - accuracy: 0.9182 - val_loss: 0.9432 - val_accuracy: 0.7161\n",
            "Epoch 12/20\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 0.1960 - accuracy: 0.9147 - val_loss: 0.9530 - val_accuracy: 0.7350\n",
            "Epoch 13/20\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 0.2359 - accuracy: 0.9017 - val_loss: 0.9998 - val_accuracy: 0.7319\n",
            "Epoch 14/20\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 0.1831 - accuracy: 0.9165 - val_loss: 1.0656 - val_accuracy: 0.7129\n",
            "Epoch 15/20\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 0.1622 - accuracy: 0.9323 - val_loss: 1.0877 - val_accuracy: 0.7192\n",
            "Epoch 16/20\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 0.1449 - accuracy: 0.9361 - val_loss: 1.1219 - val_accuracy: 0.7192\n",
            "Epoch 17/20\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 0.1383 - accuracy: 0.9382 - val_loss: 1.1472 - val_accuracy: 0.7098\n",
            "Epoch 18/20\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 0.1336 - accuracy: 0.9421 - val_loss: 1.2575 - val_accuracy: 0.7161\n",
            "Epoch 19/20\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 0.1360 - accuracy: 0.9372 - val_loss: 1.1656 - val_accuracy: 0.7161\n",
            "Epoch 20/20\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 0.1279 - accuracy: 0.9445 - val_loss: 1.2638 - val_accuracy: 0.7129\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}